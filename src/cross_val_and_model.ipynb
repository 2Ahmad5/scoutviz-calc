{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"../data/frc_data/data_2.csv\")  # Replace with your actual data file\n",
    "df.drop('team', inplace=True, axis=1)\n",
    "\n",
    "# Prepare your feature matrix X and target vector y\n",
    "X = df.drop('rank', axis=1)\n",
    "y = df['rank']\n",
    "weights = 1 / y  # Weights inverse of rank\n",
    "\n",
    "# Define the cross-validation parameters\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Loop over each fold in the cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    weights_train, weights_test = weights.iloc[train_index], weights.iloc[test_index]\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    # Update the feature importances\n",
    "    feature_importances += model.feature_importances_\n",
    "\n",
    "# Average the feature importances over all folds\n",
    "feature_importances /= kf.get_n_splits()\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Create a pandas Series with the feature importances\n",
    "importances = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "auto_csp_mult = importances.get('auto_charge_station_points')\n",
    "auto_gp_mult = importances.get('auto_grid_points')\n",
    "cup_mult = importances.get('cube_points')\n",
    "cop_mult = importances.get('cone_points')\n",
    "eg_mult = importances.get('endgame_charge_station_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2056: 0.8159460161049206,\n",
       " 254: 0.797313634663906,\n",
       " 5940: 0.7929019982888194,\n",
       " 111: 0.7779482891720839,\n",
       " 6329: 0.7768967675040874,\n",
       " 2468: 0.7669093453723091,\n",
       " 930: 0.7457061589472969,\n",
       " 4028: 0.7414849041146045,\n",
       " 4522: 0.7412779070722433,\n",
       " 118: 0.7357946811659777,\n",
       " 3538: 0.7354513032458936,\n",
       " 6036: 0.7329280347916745,\n",
       " 2075: 0.7311305149347533,\n",
       " 6328: 0.7234932215666412,\n",
       " 176: 0.7230240940816947,\n",
       " 4143: 0.7168403844975693,\n",
       " 6090: 0.7125614518734417,\n",
       " 2521: 0.7096156265956846,\n",
       " 1619: 0.7064466711749298,\n",
       " 3175: 0.7037706970424698,\n",
       " 3218: 0.7007741511936982,\n",
       " 624: 0.6997776979680267,\n",
       " 2539: 0.6986233645211527,\n",
       " 4499: 0.6981560174930643,\n",
       " 1868: 0.6963519801310358,\n",
       " 1756: 0.6939992174557316,\n",
       " 494: 0.6909233610342003,\n",
       " 1577: 0.688891277662254,\n",
       " 1771: 0.683322261453748,\n",
       " 4039: 0.6832030376781839,\n",
       " 1391: 0.6802086685638907,\n",
       " 195: 0.6799163515345015,\n",
       " 70: 0.6757803251554205,\n",
       " 1538: 0.6726150501588739,\n",
       " 1683: 0.6719295440842519,\n",
       " 359: 0.6688567696719488,\n",
       " 973: 0.6687838943290293,\n",
       " 3039: 0.6679611332704405,\n",
       " 1757: 0.6622499293980497,\n",
       " 33: 0.6609648345468843,\n",
       " 51: 0.6598022766825603,\n",
       " 1718: 0.6592604520342874,\n",
       " 694: 0.6586676675739552,\n",
       " 1768: 0.6582292002711778,\n",
       " 7890: 0.6567090676371042,\n",
       " 319: 0.655605006276709,\n",
       " 2974: 0.6548350428979752,\n",
       " 180: 0.6546524512391658,\n",
       " 3015: 0.6540855593457523,\n",
       " 1923: 0.653571486674803,\n",
       " 3655: 0.6534499130210112,\n",
       " 971: 0.6515596569204283,\n",
       " 302: 0.6514330317983479,\n",
       " 4362: 0.6494730577985116,\n",
       " 1506: 0.6454156495332988,\n",
       " 7021: 0.64487034903792,\n",
       " 972: 0.6440338962342942,\n",
       " 7558: 0.643014425446202,\n",
       " 3937: 0.6429886353843236,\n",
       " 5804: 0.6427651058386192,\n",
       " 604: 0.642677356162689,\n",
       " 7211: 0.6412810413725396,\n",
       " 7157: 0.6405885418040967,\n",
       " 5895: 0.6398117683018717,\n",
       " 148: 0.6396419847831514,\n",
       " 88: 0.6382299969562351,\n",
       " 4909: 0.637468189984135,\n",
       " 384: 0.6365745948738153,\n",
       " 3184: 0.6352969453116799,\n",
       " 5727: 0.6324146782104727,\n",
       " 78: 0.6323542302994334,\n",
       " 5152: 0.6312646141594382,\n",
       " 2642: 0.6299717730750203,\n",
       " 987: 0.628440106960419,\n",
       " 316: 0.6273316387794559,\n",
       " 5687: 0.6273299886618179,\n",
       " 6672: 0.624234457175051,\n",
       " 4607: 0.6234482686989362,\n",
       " 900: 0.6232258985780486,\n",
       " 4270: 0.6212283020453355,\n",
       " 4373: 0.6209909913573183,\n",
       " 3310: 0.6199857142694942,\n",
       " 4476: 0.6199768897166897,\n",
       " 8046: 0.6194640435434271,\n",
       " 587: 0.6188386073570908,\n",
       " 1746: 0.6186514560410695,\n",
       " 11: 0.618494856983555,\n",
       " 2992: 0.6183365142289117,\n",
       " 3990: 0.6180714916458416,\n",
       " 818: 0.6172569232984133,\n",
       " 4903: 0.6170929365159443,\n",
       " 687: 0.6164913202286626,\n",
       " 8592: 0.6158483497240235,\n",
       " 1100: 0.6157223264964421,\n",
       " 6377: 0.6146460356630857,\n",
       " 1241: 0.6139483742985135,\n",
       " 3663: 0.6127085748795623,\n",
       " 1425: 0.6125747543939387,\n",
       " 3683: 0.61208285443273,\n",
       " 5907: 0.6119555755223466,\n",
       " 2687: 0.6118383998634515,\n",
       " 4738: 0.6117800801277158,\n",
       " 325: 0.6102035796399639,\n",
       " 4253: 0.6097378118545242,\n",
       " 1501: 0.6095547928197221,\n",
       " 2399: 0.6085538087697873,\n",
       " 1574: 0.6056041286962226,\n",
       " 1339: 0.6030189694647373,\n",
       " 1189: 0.6016260207989699,\n",
       " 4131: 0.5996376373611176,\n",
       " 8177: 0.597798968192903,\n",
       " 1684: 0.5972493383178823,\n",
       " 2199: 0.5959935638406,\n",
       " 1102: 0.5956973160460959,\n",
       " 16: 0.595399258441562,\n",
       " 3620: 0.5917278215696015,\n",
       " 2067: 0.5899619286805547,\n",
       " 3339: 0.5892112371690054,\n",
       " 834: 0.5886897517723884,\n",
       " 3604: 0.5882640243961142,\n",
       " 4329: 0.5871878233399763,\n",
       " 4010: 0.5866473976148429,\n",
       " 6002: 0.5865852815880099,\n",
       " 4020: 0.5860481102465932,\n",
       " 177: 0.5855526309876233,\n",
       " 1676: 0.5853217566782112,\n",
       " 469: 0.5849550025272705,\n",
       " 5901: 0.5843206722279346,\n",
       " 3847: 0.583990957663368,\n",
       " 1599: 0.5835474778059245,\n",
       " 4635: 0.5834746745504974,\n",
       " 8426: 0.5834720140571625,\n",
       " 649: 0.5824620220071335,\n",
       " 4920: 0.5823727727099428,\n",
       " 2582: 0.5822827253033129,\n",
       " 5135: 0.5821124908668827,\n",
       " 5414: 0.581645627815842,\n",
       " 281: 0.5815731936351529,\n",
       " 8724: 0.5815371519350365,\n",
       " 6919: 0.5813796919544211,\n",
       " 1466: 0.5810096910856987,\n",
       " 5913: 0.5800877395364422,\n",
       " 955: 0.5800752928041388,\n",
       " 1403: 0.5796298502225079,\n",
       " 343: 0.5795099805089863,\n",
       " 2832: 0.5789441565411471,\n",
       " 857: 0.5778697839012923,\n",
       " 7285: 0.577036024759813,\n",
       " 1414: 0.5767698821094781,\n",
       " 1732: 0.5766053038914615,\n",
       " 9312: 0.5741196314590159,\n",
       " 5409: 0.5737996614352439,\n",
       " 449: 0.5736714951102824,\n",
       " 3478: 0.5736347307057547,\n",
       " 1730: 0.5731486973209627,\n",
       " 5411: 0.5712796922012572,\n",
       " 4561: 0.5707844312298418,\n",
       " 1540: 0.5702298092756257,\n",
       " 2240: 0.5696081373788191,\n",
       " 2607: 0.5694331497402627,\n",
       " 8085: 0.5691894767812274,\n",
       " 1714: 0.5691236951953788,\n",
       " 115: 0.5684257195732181,\n",
       " 4069: 0.5664064257060428,\n",
       " 2370: 0.5657123000921083,\n",
       " 862: 0.565438155071836,\n",
       " 422: 0.5653514805837763,\n",
       " 4976: 0.5642087986414008,\n",
       " 333: 0.5635965570468209,\n",
       " 1745: 0.56326189281619,\n",
       " 3940: 0.5625269633408075,\n",
       " 1629: 0.5619534752463212,\n",
       " 5024: 0.5605297133812724,\n",
       " 3668: 0.5592592924644908,\n",
       " 3061: 0.5583001574821159,\n",
       " 7174: 0.5582898357385145,\n",
       " 2847: 0.5568123495307332,\n",
       " 503: 0.5567953215188532,\n",
       " 4342: 0.5562570199913455,\n",
       " 85: 0.555713928782317,\n",
       " 7617: 0.5545440965376571,\n",
       " 9084: 0.5541249446215968,\n",
       " 4392: 0.5521594197631069,\n",
       " 27: 0.5490735236735884,\n",
       " 1481: 0.5485359259867193,\n",
       " 5990: 0.548475266331838,\n",
       " 9140: 0.5480523308148465,\n",
       " 360: 0.5480310844987145,\n",
       " 1468: 0.5478046364152932,\n",
       " 1561: 0.5468519142945567,\n",
       " 6045: 0.5468315609595051,\n",
       " 3767: 0.546455455883255,\n",
       " 179: 0.5461038381711667,\n",
       " 4003: 0.545716036194029,\n",
       " 7220: 0.5454264087736023,\n",
       " 3506: 0.5443372182602312,\n",
       " 6823: 0.5431126452234752,\n",
       " 1218: 0.5429527379601896,\n",
       " 1123: 0.541435639904682,\n",
       " 4766: 0.5408917734524359,\n",
       " 2106: 0.537982850825965,\n",
       " 2230: 0.5369432315264844,\n",
       " 1785: 0.5367046298100606,\n",
       " 324: 0.5365758492448972,\n",
       " 6078: 0.5362872239799923,\n",
       " 4776: 0.5360300964463358,\n",
       " 910: 0.5347312099786814,\n",
       " 3679: 0.5334906035598832,\n",
       " 4944: 0.5318891039834286,\n",
       " 2451: 0.5296623142120842,\n",
       " 4099: 0.5274069111734591,\n",
       " 3003: 0.5270494357347723,\n",
       " 484: 0.5269267548517177,\n",
       " 5232: 0.5259210057568833,\n",
       " 1816: 0.5253202233411918,\n",
       " 5505: 0.5241234519182305,\n",
       " 1156: 0.5236273566914783,\n",
       " 5006: 0.523250998507379,\n",
       " 4779: 0.5208981252556397,\n",
       " 2403: 0.5197611430599611,\n",
       " 25: 0.5180915897598701,\n",
       " 120: 0.5177697139014682,\n",
       " 2791: 0.5175487664490437,\n",
       " 5419: 0.5170908915456179,\n",
       " 1296: 0.5161297000281065,\n",
       " 5665: 0.5156427221750114,\n",
       " 5431: 0.5142808732275186,\n",
       " 1023: 0.5141803644221173,\n",
       " 1983: 0.5138496597967681,\n",
       " 3572: 0.5129484831151877,\n",
       " 6146: 0.5113570988051339,\n",
       " 597: 0.5111251835662831,\n",
       " 166: 0.5108893998395106,\n",
       " 4145: 0.5090029745418853,\n",
       " 4011: 0.5076708875749697,\n",
       " 6995: 0.507537925104528,\n",
       " 3132: 0.5056086255648938,\n",
       " 2202: 0.5046666418434793,\n",
       " 3142: 0.5014331744670262,\n",
       " 9125: 0.5012343506685341,\n",
       " 6348: 0.5006845948724589,\n",
       " 3322: 0.5006110331039079,\n",
       " 548: 0.4997975673239785,\n",
       " 5050: 0.4986009925397446,\n",
       " 5434: 0.4953277086722601,\n",
       " 9176: 0.49409100450292953,\n",
       " 3161: 0.49295953591897823,\n",
       " 1025: 0.49282546487040335,\n",
       " 8393: 0.49178302233409493,\n",
       " 7444: 0.491198126476777,\n",
       " 135: 0.49064328131869334,\n",
       " 7563: 0.486943842508876,\n",
       " 3616: 0.48578895966507685,\n",
       " 4122: 0.48577079439647247,\n",
       " 4419: 0.4843172292805015,\n",
       " 4905: 0.48319794965720914,\n",
       " 8576: 0.482444025991209,\n",
       " 1625: 0.48242237999175097,\n",
       " 4663: 0.4809242332819138,\n",
       " 7565: 0.4803283237214586,\n",
       " 2638: 0.48008850995863106,\n",
       " 1072: 0.47750799343889133,\n",
       " 2960: 0.47469572974237445,\n",
       " 5654: 0.47066354617113265,\n",
       " 2429: 0.47043968525116686,\n",
       " 4624: 0.4701402948650025,\n",
       " 9015: 0.46933576733834165,\n",
       " 4276: 0.46832016574121954,\n",
       " 3374: 0.46539769785390617,\n",
       " 5635: 0.46432442620958936,\n",
       " 2220: 0.46052503496992664,\n",
       " 1802: 0.4594825378213915,\n",
       " 7072: 0.45912363043292675,\n",
       " 3932: 0.45585782305941547,\n",
       " 9101: 0.45230726317492775,\n",
       " 7460: 0.4498570636935227,\n",
       " 6352: 0.4495733935497698,\n",
       " 5618: 0.44704314513229326,\n",
       " 5274: 0.4453935852228535,\n",
       " 6413: 0.4446114196893724,\n",
       " 6420: 0.4428736642190414,\n",
       " 3749: 0.44143141385328105,\n",
       " 2170: 0.44051286782525656,\n",
       " 2059: 0.43746698141111495,\n",
       " 4293: 0.4374159783577036,\n",
       " 1836: 0.4342869824861755,\n",
       " 9092: 0.4335093633424016,\n",
       " 8808: 0.43344691645964123,\n",
       " 3130: 0.43226264320515756,\n",
       " 2875: 0.43098172588276457,\n",
       " 8847: 0.4296168342796076,\n",
       " 9067: 0.4286523616677044,\n",
       " 7042: 0.4273755849362929,\n",
       " 2506: 0.42710390781435276,\n",
       " 8729: 0.42674076259890625,\n",
       " 399: 0.4264669821675856,\n",
       " 668: 0.4190125609897071,\n",
       " 2457: 0.4162641572000525,\n",
       " 5883: 0.4157361632282295,\n",
       " 2945: 0.41493242663184526,\n",
       " 5338: 0.40667347030399825,\n",
       " 4: 0.3993584712142558,\n",
       " 217: 0.3983439962032326,\n",
       " 5653: 0.39480356621535767,\n",
       " 9038: 0.3906430516272759,\n",
       " 6865: 0.39025323526603384,\n",
       " 6647: 0.39001051393520364,\n",
       " 5089: 0.38948020250628396,\n",
       " 533: 0.38770340429494365,\n",
       " 2637: 0.38729408907014107,\n",
       " 2635: 0.3820509764890637,\n",
       " 2656: 0.38022444187970544,\n",
       " 4598: 0.37706078090589384,\n",
       " 9022: 0.3762653024118571,\n",
       " 4112: 0.37407043077732316,\n",
       " 3284: 0.37400375251663576,\n",
       " 8739: 0.3706212005210394,\n",
       " 9016: 0.3700044387685834,\n",
       " 6833: 0.3698808202544239,\n",
       " 9023: 0.3689034994438578,\n",
       " 9008: 0.3674467446406031,\n",
       " 6657: 0.3641160143965003,\n",
       " 9068: 0.3619840947606708,\n",
       " 840: 0.36095340695813677,\n",
       " 9134: 0.3607917632127742,\n",
       " 7428: 0.3574751939649119,\n",
       " 5172: 0.35737712951051226,\n",
       " 7748: 0.3552117738660862,\n",
       " 3008: 0.353891914611567,\n",
       " 3489: 0.3526134425064691,\n",
       " 2359: 0.34684919413022197,\n",
       " 1700: 0.34187944009396204,\n",
       " 2080: 0.34178672130193594,\n",
       " 5809: 0.33448643132953476,\n",
       " 6358: 0.3331168230492638,\n",
       " 4175: 0.33261545781726376,\n",
       " 6868: 0.3321741329610819,\n",
       " 9168: 0.32974612680857424,\n",
       " 4788: 0.3260315457474477,\n",
       " 9219: 0.3259281155685176,\n",
       " 5553: 0.32589486289076963,\n",
       " 9180: 0.32585877581721906,\n",
       " 8717: 0.32124382991980854,\n",
       " 3256: 0.31862463459696205,\n",
       " 5993: 0.3140378592957592,\n",
       " 2489: 0.31393630013864104,\n",
       " 8711: 0.30994083869137773,\n",
       " 9075: 0.3046451484101726,\n",
       " 9213: 0.30317915998158407,\n",
       " 8016: 0.3026386778501977,\n",
       " 6586: 0.3020807600699998,\n",
       " 6998: 0.3015240640294263,\n",
       " 9126: 0.3002828297238594,\n",
       " 8159: 0.29994570769740864,\n",
       " 7018: 0.29860987705572845,\n",
       " 9085: 0.29652115643014065,\n",
       " 6817: 0.2949754713503722,\n",
       " 1884: 0.2932490352719251,\n",
       " 3653: 0.2921246487925527,\n",
       " 4693: 0.2795079005582124,\n",
       " 6909: 0.27630815872088405,\n",
       " 3459: 0.27373402238992117,\n",
       " 8871: 0.27230703545598,\n",
       " 28: 0.2684400329537284,\n",
       " 9062: 0.265967186001289,\n",
       " 9007: 0.265565255293977,\n",
       " 9037: 0.2650144971141156,\n",
       " 7534: 0.2645765272466763,\n",
       " 5988: 0.257738793996514,\n",
       " 9030: 0.25763796292305924,\n",
       " 3341: 0.2551571764145969,\n",
       " 9152: 0.25095816891888434,\n",
       " 3753: 0.2486649887258009,\n",
       " 6652: 0.24840384335728222,\n",
       " 3794: 0.24835039904781803,\n",
       " 9076: 0.24172490285910248,\n",
       " 2905: 0.23896598719494708,\n",
       " 9059: 0.23546582274689395,\n",
       " 9244: 0.22255216447117268,\n",
       " 3526: 0.22231408025547905,\n",
       " 9148: 0.21691817195096824,\n",
       " 8015: 0.20978288466662268,\n",
       " 6606: 0.20700631467641736,\n",
       " 9019: 0.16457688922844507,\n",
       " 5045: 0.16337835322030508,\n",
       " 9153: 0.16102574334114148,\n",
       " 6431: 0.11393403922679465}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/frc_data/joh_mil_cur_arc_new.csv\")\n",
    "\n",
    "scores = {}\n",
    "count = 0\n",
    "for x in df['team']:\n",
    "    \n",
    "    scores[x] = (auto_csp_mult * df['norm_a_c_s_p'][count]) + (auto_gp_mult * df['norm_a_g_p'][count]) + (cup_mult * df['norm_c_p'][count]) + (cop_mult * df['norm_co_p'][count]) + (eg_mult * df['norm_eg_p'][count])\n",
    "    count += 1\n",
    "\n",
    "{k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     red_team  blue_team  target\n",
      "0    1.974034   1.839258       1\n",
      "1    1.974034   1.839258       0\n",
      "2    1.974034   1.839258       0\n",
      "3    1.488090   1.512618       1\n",
      "4    1.666460   1.178957       0\n",
      "..        ...        ...     ...\n",
      "721  1.880955   1.791924       0\n",
      "722  1.825385   1.755986       1\n",
      "723  2.026924   1.924914       1\n",
      "724  1.706152   1.841136       1\n",
      "725  2.026924   1.755986       0\n",
      "\n",
      "[726 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "red_team, blue_team, won = [], [], []\n",
    "\n",
    "jqd = pd.read_csv(\"../data/frc_data/all_qual_data.csv\")\n",
    "\n",
    "iterate = 0\n",
    "\n",
    "for l in range(len(jqd['red_1'])):\n",
    "\n",
    "    red_team.append(scores[jqd[\"red_1\"][iterate]] + scores[jqd[\"red_2\"][iterate]] + scores[jqd[\"red_3\"][iterate]])\n",
    "    blue_team.append(scores[jqd[\"blue_1\"][iterate]] + scores[jqd[\"blue_2\"][iterate]] + scores[jqd[\"blue_3\"][iterate]])\n",
    "    if(jqd['win'][iterate] == \"blue\"):\n",
    "        won.append(1)\n",
    "    else:\n",
    "        won.append(0)\n",
    "    \n",
    "    iterate += 1\n",
    "\n",
    "feature_data = pd.DataFrame({\n",
    "    'red_team': red_team,\n",
    "    'blue_team': blue_team\n",
    "})\n",
    "\n",
    "# Create a Series for the target data\n",
    "target_data = pd.Series(won, name='target')\n",
    "\n",
    "# Combine into one DataFrame if needed\n",
    "combined_data = feature_data.assign(target=target_data)\n",
    "\n",
    "# Now combined_data is a DataFrame ready for use in machine learning\n",
    "print(combined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match #1: Predicted Class: 1, Actual Class: 0, Probability of being 1: 66.11%\n",
      "Match #2: Predicted Class: 1, Actual Class: 1, Probability of being 1: 61.63%\n",
      "Match #3: Predicted Class: 0, Actual Class: 1, Probability of being 1: 48.78%\n",
      "Match #4: Predicted Class: 1, Actual Class: 0, Probability of being 1: 65.90%\n",
      "Match #5: Predicted Class: 1, Actual Class: 1, Probability of being 1: 92.86%\n",
      "Match #6: Predicted Class: 1, Actual Class: 1, Probability of being 1: 89.85%\n",
      "Match #7: Predicted Class: 0, Actual Class: 0, Probability of being 1: 26.49%\n",
      "Match #8: Predicted Class: 1, Actual Class: 0, Probability of being 1: 50.52%\n",
      "Match #9: Predicted Class: 0, Actual Class: 1, Probability of being 1: 31.94%\n",
      "Match #10: Predicted Class: 1, Actual Class: 1, Probability of being 1: 71.04%\n",
      "Match #11: Predicted Class: 0, Actual Class: 0, Probability of being 1: 8.51%\n",
      "Match #12: Predicted Class: 1, Actual Class: 1, Probability of being 1: 91.57%\n",
      "Match #13: Predicted Class: 1, Actual Class: 1, Probability of being 1: 81.71%\n",
      "Match #14: Predicted Class: 0, Actual Class: 0, Probability of being 1: 17.68%\n",
      "Match #15: Predicted Class: 0, Actual Class: 1, Probability of being 1: 45.01%\n",
      "Match #16: Predicted Class: 1, Actual Class: 0, Probability of being 1: 61.71%\n",
      "Match #17: Predicted Class: 1, Actual Class: 0, Probability of being 1: 88.27%\n",
      "Match #18: Predicted Class: 1, Actual Class: 1, Probability of being 1: 81.60%\n",
      "Match #19: Predicted Class: 1, Actual Class: 1, Probability of being 1: 93.88%\n",
      "Match #20: Predicted Class: 1, Actual Class: 1, Probability of being 1: 91.52%\n",
      "Match #21: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.35%\n",
      "Match #22: Predicted Class: 1, Actual Class: 1, Probability of being 1: 97.57%\n",
      "Match #23: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.72%\n",
      "Match #24: Predicted Class: 0, Actual Class: 0, Probability of being 1: 3.02%\n",
      "Match #25: Predicted Class: 1, Actual Class: 1, Probability of being 1: 98.36%\n",
      "Match #26: Predicted Class: 1, Actual Class: 1, Probability of being 1: 93.36%\n",
      "Match #27: Predicted Class: 1, Actual Class: 1, Probability of being 1: 95.28%\n",
      "Match #28: Predicted Class: 0, Actual Class: 0, Probability of being 1: 10.93%\n",
      "Match #29: Predicted Class: 1, Actual Class: 1, Probability of being 1: 77.28%\n",
      "Match #30: Predicted Class: 1, Actual Class: 1, Probability of being 1: 68.34%\n",
      "Match #31: Predicted Class: 0, Actual Class: 0, Probability of being 1: 15.53%\n",
      "Match #32: Predicted Class: 1, Actual Class: 1, Probability of being 1: 97.83%\n",
      "Match #33: Predicted Class: 1, Actual Class: 1, Probability of being 1: 78.74%\n",
      "Match #34: Predicted Class: 1, Actual Class: 1, Probability of being 1: 53.46%\n",
      "Match #35: Predicted Class: 0, Actual Class: 1, Probability of being 1: 45.36%\n",
      "Match #36: Predicted Class: 0, Actual Class: 0, Probability of being 1: 34.88%\n",
      "Match #37: Predicted Class: 0, Actual Class: 0, Probability of being 1: 39.27%\n",
      "Match #38: Predicted Class: 1, Actual Class: 1, Probability of being 1: 98.74%\n",
      "Match #39: Predicted Class: 0, Actual Class: 0, Probability of being 1: 6.49%\n",
      "Match #40: Predicted Class: 0, Actual Class: 0, Probability of being 1: 31.48%\n",
      "Match #41: Predicted Class: 1, Actual Class: 1, Probability of being 1: 54.01%\n",
      "Match #42: Predicted Class: 1, Actual Class: 1, Probability of being 1: 81.13%\n",
      "Match #43: Predicted Class: 0, Actual Class: 0, Probability of being 1: 39.35%\n",
      "Match #44: Predicted Class: 0, Actual Class: 0, Probability of being 1: 31.07%\n",
      "Match #45: Predicted Class: 1, Actual Class: 1, Probability of being 1: 79.45%\n",
      "Match #46: Predicted Class: 0, Actual Class: 0, Probability of being 1: 15.47%\n",
      "Match #47: Predicted Class: 0, Actual Class: 0, Probability of being 1: 31.35%\n",
      "Match #48: Predicted Class: 0, Actual Class: 1, Probability of being 1: 47.02%\n",
      "Match #49: Predicted Class: 0, Actual Class: 0, Probability of being 1: 29.42%\n",
      "Match #50: Predicted Class: 1, Actual Class: 1, Probability of being 1: 65.91%\n",
      "Match #51: Predicted Class: 0, Actual Class: 0, Probability of being 1: 6.43%\n",
      "Match #52: Predicted Class: 1, Actual Class: 1, Probability of being 1: 62.44%\n",
      "Match #53: Predicted Class: 1, Actual Class: 1, Probability of being 1: 83.55%\n",
      "Match #54: Predicted Class: 0, Actual Class: 1, Probability of being 1: 48.67%\n",
      "Match #55: Predicted Class: 0, Actual Class: 0, Probability of being 1: 1.15%\n",
      "Match #56: Predicted Class: 0, Actual Class: 1, Probability of being 1: 37.36%\n",
      "Match #57: Predicted Class: 1, Actual Class: 1, Probability of being 1: 99.07%\n",
      "Match #58: Predicted Class: 0, Actual Class: 0, Probability of being 1: 31.87%\n",
      "Match #59: Predicted Class: 0, Actual Class: 0, Probability of being 1: 26.54%\n",
      "Match #60: Predicted Class: 1, Actual Class: 1, Probability of being 1: 76.60%\n",
      "Match #61: Predicted Class: 0, Actual Class: 0, Probability of being 1: 17.30%\n",
      "Match #62: Predicted Class: 1, Actual Class: 1, Probability of being 1: 74.84%\n",
      "Match #63: Predicted Class: 0, Actual Class: 0, Probability of being 1: 13.59%\n",
      "Match #64: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.92%\n",
      "Match #65: Predicted Class: 0, Actual Class: 0, Probability of being 1: 31.79%\n",
      "Match #66: Predicted Class: 1, Actual Class: 1, Probability of being 1: 93.54%\n",
      "Match #67: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.33%\n",
      "Match #68: Predicted Class: 0, Actual Class: 0, Probability of being 1: 30.34%\n",
      "Match #69: Predicted Class: 0, Actual Class: 0, Probability of being 1: 34.81%\n",
      "Match #70: Predicted Class: 0, Actual Class: 1, Probability of being 1: 35.96%\n",
      "Match #71: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.12%\n",
      "Match #72: Predicted Class: 1, Actual Class: 1, Probability of being 1: 96.85%\n",
      "Match #73: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.13%\n",
      "Match #74: Predicted Class: 1, Actual Class: 1, Probability of being 1: 70.77%\n",
      "Match #75: Predicted Class: 0, Actual Class: 0, Probability of being 1: 10.66%\n",
      "Match #76: Predicted Class: 1, Actual Class: 0, Probability of being 1: 69.43%\n",
      "Match #77: Predicted Class: 0, Actual Class: 0, Probability of being 1: 1.09%\n",
      "Match #78: Predicted Class: 0, Actual Class: 0, Probability of being 1: 4.97%\n",
      "Match #79: Predicted Class: 1, Actual Class: 0, Probability of being 1: 67.70%\n",
      "Match #80: Predicted Class: 0, Actual Class: 0, Probability of being 1: 27.76%\n",
      "Match #81: Predicted Class: 0, Actual Class: 0, Probability of being 1: 2.11%\n",
      "Match #82: Predicted Class: 0, Actual Class: 1, Probability of being 1: 15.11%\n",
      "Match #83: Predicted Class: 1, Actual Class: 0, Probability of being 1: 64.53%\n",
      "Match #84: Predicted Class: 1, Actual Class: 1, Probability of being 1: 97.89%\n",
      "Match #85: Predicted Class: 1, Actual Class: 1, Probability of being 1: 84.81%\n",
      "Match #86: Predicted Class: 0, Actual Class: 0, Probability of being 1: 28.75%\n",
      "Match #87: Predicted Class: 1, Actual Class: 0, Probability of being 1: 58.18%\n",
      "Match #88: Predicted Class: 0, Actual Class: 0, Probability of being 1: 29.31%\n",
      "Match #89: Predicted Class: 1, Actual Class: 1, Probability of being 1: 62.88%\n",
      "Match #90: Predicted Class: 1, Actual Class: 1, Probability of being 1: 93.20%\n",
      "Match #91: Predicted Class: 1, Actual Class: 1, Probability of being 1: 70.51%\n",
      "Match #92: Predicted Class: 1, Actual Class: 1, Probability of being 1: 95.55%\n",
      "Match #93: Predicted Class: 0, Actual Class: 1, Probability of being 1: 35.56%\n",
      "Match #94: Predicted Class: 0, Actual Class: 0, Probability of being 1: 5.15%\n",
      "Match #95: Predicted Class: 1, Actual Class: 1, Probability of being 1: 53.66%\n",
      "Match #96: Predicted Class: 0, Actual Class: 0, Probability of being 1: 13.57%\n",
      "Match #97: Predicted Class: 0, Actual Class: 0, Probability of being 1: 20.53%\n",
      "Match #98: Predicted Class: 0, Actual Class: 0, Probability of being 1: 1.07%\n",
      "Match #99: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.34%\n",
      "Match #100: Predicted Class: 0, Actual Class: 0, Probability of being 1: 10.49%\n",
      "Match #101: Predicted Class: 1, Actual Class: 1, Probability of being 1: 87.40%\n",
      "Match #102: Predicted Class: 1, Actual Class: 1, Probability of being 1: 63.06%\n",
      "Match #103: Predicted Class: 1, Actual Class: 1, Probability of being 1: 97.08%\n",
      "Match #104: Predicted Class: 0, Actual Class: 0, Probability of being 1: 20.98%\n",
      "Match #105: Predicted Class: 0, Actual Class: 1, Probability of being 1: 35.66%\n",
      "Match #106: Predicted Class: 0, Actual Class: 0, Probability of being 1: 11.18%\n",
      "Match #107: Predicted Class: 1, Actual Class: 0, Probability of being 1: 88.40%\n",
      "Match #108: Predicted Class: 1, Actual Class: 1, Probability of being 1: 57.92%\n",
      "Match #109: Predicted Class: 0, Actual Class: 0, Probability of being 1: 28.09%\n",
      "Match #110: Predicted Class: 1, Actual Class: 0, Probability of being 1: 50.09%\n",
      "Match #111: Predicted Class: 0, Actual Class: 1, Probability of being 1: 28.80%\n",
      "Match #112: Predicted Class: 1, Actual Class: 0, Probability of being 1: 57.24%\n",
      "Match #113: Predicted Class: 0, Actual Class: 0, Probability of being 1: 20.18%\n",
      "Match #114: Predicted Class: 0, Actual Class: 0, Probability of being 1: 18.85%\n",
      "Match #115: Predicted Class: 1, Actual Class: 0, Probability of being 1: 57.31%\n",
      "Match #116: Predicted Class: 0, Actual Class: 0, Probability of being 1: 21.50%\n",
      "Match #117: Predicted Class: 1, Actual Class: 0, Probability of being 1: 59.40%\n",
      "Match #118: Predicted Class: 0, Actual Class: 0, Probability of being 1: 3.91%\n",
      "Match #119: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.65%\n",
      "Match #120: Predicted Class: 0, Actual Class: 0, Probability of being 1: 48.87%\n",
      "Match #121: Predicted Class: 0, Actual Class: 0, Probability of being 1: 45.88%\n",
      "Match #122: Predicted Class: 0, Actual Class: 1, Probability of being 1: 20.09%\n",
      "Match #123: Predicted Class: 1, Actual Class: 1, Probability of being 1: 73.65%\n",
      "Match #124: Predicted Class: 0, Actual Class: 0, Probability of being 1: 3.78%\n",
      "Match #125: Predicted Class: 0, Actual Class: 0, Probability of being 1: 15.39%\n",
      "Match #126: Predicted Class: 0, Actual Class: 0, Probability of being 1: 27.71%\n",
      "Match #127: Predicted Class: 1, Actual Class: 1, Probability of being 1: 77.26%\n",
      "Match #128: Predicted Class: 0, Actual Class: 0, Probability of being 1: 15.55%\n",
      "Match #129: Predicted Class: 0, Actual Class: 0, Probability of being 1: 30.60%\n",
      "Match #130: Predicted Class: 0, Actual Class: 0, Probability of being 1: 0.76%\n",
      "Match #131: Predicted Class: 0, Actual Class: 0, Probability of being 1: 21.57%\n",
      "Match #132: Predicted Class: 1, Actual Class: 1, Probability of being 1: 71.45%\n",
      "Match #133: Predicted Class: 1, Actual Class: 1, Probability of being 1: 55.02%\n",
      "Match #134: Predicted Class: 0, Actual Class: 0, Probability of being 1: 41.36%\n",
      "Match #135: Predicted Class: 0, Actual Class: 1, Probability of being 1: 12.05%\n",
      "Match #136: Predicted Class: 0, Actual Class: 0, Probability of being 1: 11.76%\n",
      "Match #137: Predicted Class: 1, Actual Class: 0, Probability of being 1: 62.01%\n",
      "Match #138: Predicted Class: 1, Actual Class: 1, Probability of being 1: 81.45%\n",
      "Match #139: Predicted Class: 1, Actual Class: 1, Probability of being 1: 96.95%\n",
      "Match #140: Predicted Class: 1, Actual Class: 1, Probability of being 1: 88.21%\n",
      "Match #141: Predicted Class: 0, Actual Class: 0, Probability of being 1: 0.78%\n",
      "Match #142: Predicted Class: 0, Actual Class: 0, Probability of being 1: 17.34%\n",
      "Match #143: Predicted Class: 0, Actual Class: 0, Probability of being 1: 21.38%\n",
      "Match #144: Predicted Class: 0, Actual Class: 0, Probability of being 1: 20.40%\n",
      "Match #145: Predicted Class: 1, Actual Class: 0, Probability of being 1: 57.78%\n",
      "Match #146: Predicted Class: 1, Actual Class: 1, Probability of being 1: 99.38%\n",
      "Match #147: Predicted Class: 1, Actual Class: 1, Probability of being 1: 88.49%\n",
      "Match #148: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.95%\n",
      "Match #149: Predicted Class: 0, Actual Class: 0, Probability of being 1: 6.35%\n",
      "Match #150: Predicted Class: 0, Actual Class: 0, Probability of being 1: 2.19%\n",
      "Match #151: Predicted Class: 0, Actual Class: 1, Probability of being 1: 32.52%\n",
      "Match #152: Predicted Class: 1, Actual Class: 1, Probability of being 1: 88.63%\n",
      "Match #153: Predicted Class: 0, Actual Class: 1, Probability of being 1: 44.18%\n",
      "Match #154: Predicted Class: 0, Actual Class: 0, Probability of being 1: 19.32%\n",
      "Match #155: Predicted Class: 1, Actual Class: 1, Probability of being 1: 70.11%\n",
      "Match #156: Predicted Class: 1, Actual Class: 1, Probability of being 1: 91.45%\n",
      "Match #157: Predicted Class: 1, Actual Class: 0, Probability of being 1: 77.18%\n",
      "Match #158: Predicted Class: 1, Actual Class: 0, Probability of being 1: 86.10%\n",
      "Match #159: Predicted Class: 0, Actual Class: 0, Probability of being 1: 14.34%\n",
      "Match #160: Predicted Class: 1, Actual Class: 0, Probability of being 1: 55.24%\n",
      "Match #161: Predicted Class: 1, Actual Class: 1, Probability of being 1: 88.38%\n",
      "Match #162: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.92%\n",
      "Match #163: Predicted Class: 0, Actual Class: 0, Probability of being 1: 7.77%\n",
      "Match #164: Predicted Class: 1, Actual Class: 1, Probability of being 1: 72.10%\n",
      "Match #165: Predicted Class: 1, Actual Class: 1, Probability of being 1: 99.31%\n",
      "Match #166: Predicted Class: 0, Actual Class: 1, Probability of being 1: 29.76%\n",
      "Match #167: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.37%\n",
      "Match #168: Predicted Class: 1, Actual Class: 1, Probability of being 1: 94.71%\n",
      "Match #169: Predicted Class: 1, Actual Class: 1, Probability of being 1: 83.69%\n",
      "Match #170: Predicted Class: 0, Actual Class: 0, Probability of being 1: 0.86%\n",
      "Match #171: Predicted Class: 1, Actual Class: 0, Probability of being 1: 56.65%\n",
      "Match #172: Predicted Class: 1, Actual Class: 1, Probability of being 1: 93.91%\n",
      "Match #173: Predicted Class: 1, Actual Class: 0, Probability of being 1: 64.09%\n",
      "Match #174: Predicted Class: 0, Actual Class: 1, Probability of being 1: 30.71%\n",
      "Match #175: Predicted Class: 1, Actual Class: 1, Probability of being 1: 85.98%\n",
      "Match #176: Predicted Class: 0, Actual Class: 0, Probability of being 1: 39.53%\n",
      "Match #177: Predicted Class: 0, Actual Class: 0, Probability of being 1: 12.35%\n",
      "Match #178: Predicted Class: 1, Actual Class: 1, Probability of being 1: 95.48%\n",
      "Match #179: Predicted Class: 1, Actual Class: 1, Probability of being 1: 50.63%\n",
      "Match #180: Predicted Class: 0, Actual Class: 1, Probability of being 1: 16.08%\n",
      "Match #181: Predicted Class: 0, Actual Class: 0, Probability of being 1: 35.18%\n",
      "Match #182: Predicted Class: 0, Actual Class: 0, Probability of being 1: 12.30%\n"
     ]
    }
   ],
   "source": [
    "X = feature_data\n",
    "y = combined_data['target']\n",
    "\n",
    "combined_data['target'].value_counts()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=.25, random_state=0)\n",
    "\n",
    "ss_train = StandardScaler()\n",
    "X_train = ss_train.fit_transform(X_train)\n",
    "\n",
    "ss_test = StandardScaler()\n",
    "X_test = ss_test.fit_transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prob_predictions = model.predict_proba(X_test)\n",
    "class_predictions = model.predict(X_test)\n",
    "\n",
    "# Loop through the predictions and print required information\n",
    "for i, (prob, prediction, actual) in enumerate(zip(prob_predictions, class_predictions, y_test), start=1):\n",
    "    # Probability of the positive class (assuming it is the second one)\n",
    "    prob_of_one = prob[1] * 100\n",
    "    print(f'Match #{i}: Predicted Class: {prediction}, Actual Class: {actual}, Probability of being 1: {prob_of_one:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive(TP)  =  69\n",
      "False Positive(FP) =  21\n",
      "True Negative(TN)  =  73\n",
      "False Negative(FN) =  19\n",
      "Accuracy of the binary classifier = 0.780\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, class_predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, class_predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "print('Accuracy of the binary classifier = {:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1323: 0.946382804,\n",
       " 2046: 0.8917746075398001,\n",
       " 3357: 0.8161061921170001,\n",
       " 7407: 0.8071301222830001,\n",
       " 4391: 0.7815731428286,\n",
       " 1727: 0.7810660209428,\n",
       " 2930: 0.7722730990039001,\n",
       " 2338: 0.7555148212573,\n",
       " 1987: 0.7481121404458,\n",
       " 2481: 0.736865796372,\n",
       " 341: 0.7311524088431,\n",
       " 4414: 0.7190716155307,\n",
       " 876: 0.717220559645,\n",
       " 7457: 0.7170056608375,\n",
       " 488: 0.7101935825374,\n",
       " 2052: 0.7085721917039,\n",
       " 1706: 0.7054546275526,\n",
       " 245: 0.7017427944043,\n",
       " 3636: 0.6986846510378001,\n",
       " 4265: 0.6973557651632,\n",
       " 1986: 0.6834936184809001,\n",
       " 1796: 0.6639383769548,\n",
       " 610: 0.6457679710516,\n",
       " 4682: 0.6423831773794,\n",
       " 573: 0.6414070169232999,\n",
       " 4336: 0.6381148041888,\n",
       " 2169: 0.630042503817,\n",
       " 2609: 0.6175936722669,\n",
       " 8019: 0.6110587430848,\n",
       " 4096: 0.607124299283,\n",
       " 999: 0.6021655660887,\n",
       " 226: 0.5998412531784,\n",
       " 1807: 0.5997045745914,\n",
       " 107: 0.5783955715403,\n",
       " 498: 0.5734942985878,\n",
       " 6357: 0.5708526490142,\n",
       " 5675: 0.5687504181212,\n",
       " 2495: 0.5656325803102,\n",
       " 696: 0.5639320544218,\n",
       " 8736: 0.5599884765158001,\n",
       " 1792: 0.5494945632855001,\n",
       " 5526: 0.5421220879466001,\n",
       " 4400: 0.5367222412127,\n",
       " 4213: 0.5341233741411001,\n",
       " 5667: 0.5270837488793,\n",
       " 2096: 0.5210448373672,\n",
       " 68: 0.5080568538147,\n",
       " 353: 0.48247486128100003,\n",
       " 9260: 0.4778414884349,\n",
       " 365: 0.47703275345289997,\n",
       " 201: 0.4561262888561,\n",
       " 190: 0.4546778734545,\n",
       " 9182: 0.44956418047970004,\n",
       " 6721: 0.4486076161065,\n",
       " 6429: 0.4386835715699,\n",
       " 772: 0.426802233611,\n",
       " 3654: 0.42591457985510006,\n",
       " 4135: 0.4040890208354,\n",
       " 6024: 0.39465786070819997,\n",
       " 9071: 0.3935867935057,\n",
       " 9082: 0.38858743264376006,\n",
       " 9118: 0.38714944811298,\n",
       " 6517: 0.3869150998587,\n",
       " 9202: 0.38162591921725997,\n",
       " 4930: 0.37262078088460004,\n",
       " 3189: 0.35358837074820004,\n",
       " 4450: 0.3502827800111,\n",
       " 1311: 0.350049613932,\n",
       " 1799: 0.34636949926349997,\n",
       " 2341: 0.33780491076235003,\n",
       " 2718: 0.33212302365164,\n",
       " 5587: 0.2934812027171,\n",
       " 6832: 0.281772531258494,\n",
       " 5816: 0.24849218855594002,\n",
       " 3019: 0.22982438970907998,\n",
       " 5648: 0.207732372984,\n",
       " 5123: 0.20340160196728002}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/frc_data/2023hop_norm.csv\")\n",
    "\n",
    "scores = {}\n",
    "count = 0\n",
    "for x in df['team']:\n",
    "    \n",
    "    scores[x] = (.182 * df['norm_a_c_s_p'][count]) + (.221 * df['norm_a_g_p'][count]) + (.174 * df['norm_c_p'][count]) + (.222 * df['norm_co_p'][count]) + (.202 * df['norm_eg_p'][count])\n",
    "    count += 1\n",
    "\n",
    "{k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy on the new data: 0.8541666667\n",
      "     Match Number  Actual Outcome  Predicted Outcome  Probability of Outcome\n",
      "0               0               0                  0                0.180817\n",
      "1               1               0                  0                0.180817\n",
      "2               2               1                  1                0.714029\n",
      "3               3               1                  1                0.994816\n",
      "4               4               1                  1                0.994838\n",
      "5               5               0                  0                0.021525\n",
      "6               6               0                  0                0.252707\n",
      "7               7               1                  1                0.973739\n",
      "8               8               1                  0                0.231234\n",
      "9               9               1                  1                0.762600\n",
      "10             10               0                  0                0.119657\n",
      "11             11               1                  1                0.963426\n",
      "12             12               0                  0                0.029910\n",
      "13             13               0                  1                0.778738\n",
      "14             14               0                  1                0.786578\n",
      "15             15               0                  1                0.870498\n",
      "16             16               0                  1                0.630960\n",
      "17             17               1                  1                0.991258\n",
      "18             18               0                  0                0.316534\n",
      "19             19               1                  1                0.578572\n",
      "20             20               1                  1                0.909465\n",
      "21             21               1                  1                0.935947\n",
      "22             22               1                  1                0.958778\n",
      "23             23               0                  1                0.678464\n",
      "24             24               0                  0                0.011446\n",
      "25             25               1                  1                0.997791\n",
      "26             26               0                  0                0.080832\n",
      "27             27               1                  1                0.992056\n",
      "28             28               1                  1                0.846422\n",
      "29             29               0                  0                0.134212\n",
      "30             30               0                  0                0.000650\n",
      "31             31               0                  0                0.274468\n",
      "32             32               1                  1                0.957611\n",
      "33             33               1                  1                0.875813\n",
      "34             34               0                  0                0.321365\n",
      "35             35               1                  0                0.302841\n",
      "36             36               0                  0                0.447267\n",
      "37             37               1                  1                0.958022\n",
      "38             38               0                  0                0.474379\n",
      "39             39               0                  0                0.067483\n",
      "40             40               1                  1                0.988977\n",
      "41             41               1                  1                0.965360\n",
      "42             42               1                  1                0.781150\n",
      "43             43               0                  0                0.051193\n",
      "44             44               1                  1                0.975436\n",
      "45             45               1                  1                0.971375\n",
      "46             46               1                  1                0.973405\n",
      "47             47               1                  1                0.971430\n",
      "48             48               0                  0                0.011118\n",
      "49             49               1                  1                0.956526\n",
      "50             50               0                  0                0.162803\n",
      "51             51               1                  1                0.672445\n",
      "52             52               0                  0                0.096275\n",
      "53             53               1                  1                0.824653\n",
      "54             54               1                  1                0.953682\n",
      "55             55               1                  1                0.500536\n",
      "56             56               0                  0                0.088681\n",
      "57             57               1                  1                0.572417\n",
      "58             58               1                  1                0.982266\n",
      "59             59               0                  0                0.026306\n",
      "60             60               0                  0                0.325928\n",
      "61             61               1                  0                0.156186\n",
      "62             62               0                  0                0.158456\n",
      "63             63               0                  0                0.015603\n",
      "64             64               0                  1                0.640130\n",
      "65             65               1                  1                0.989970\n",
      "66             66               1                  1                0.725372\n",
      "67             67               1                  0                0.161169\n",
      "68             68               0                  0                0.354913\n",
      "69             69               0                  0                0.016084\n",
      "70             70               0                  0                0.031501\n",
      "71             71               0                  0                0.342648\n",
      "72             72               1                  1                0.754039\n",
      "73             73               1                  1                0.988864\n",
      "74             74               1                  1                0.930344\n",
      "75             75               0                  0                0.292445\n",
      "76             76               0                  0                0.029395\n",
      "77             77               1                  0                0.430632\n",
      "78             78               0                  0                0.299372\n",
      "79             79               0                  0                0.044770\n",
      "80             80               0                  0                0.146151\n",
      "81             81               0                  0                0.491502\n",
      "82             82               1                  1                0.755374\n",
      "83             83               1                  1                0.996471\n",
      "84             84               0                  0                0.026583\n",
      "85             85               0                  0                0.230665\n",
      "86             86               1                  1                0.997600\n",
      "87             87               0                  0                0.008503\n",
      "88             88               0                  0                0.000500\n",
      "89             89               0                  0                0.003171\n",
      "90             90               0                  1                0.593267\n",
      "91             91               0                  0                0.095025\n",
      "92             92               1                  0                0.442450\n",
      "93             93               0                  0                0.023593\n",
      "94             94               0                  1                0.862362\n",
      "95             95               1                  1                0.908069\n",
      "96             96               0                  0                0.050510\n",
      "97             97               1                  1                0.980588\n",
      "98             98               0                  0                0.022633\n",
      "99             99               1                  1                0.720326\n",
      "100           100               0                  0                0.003964\n",
      "101           101               1                  1                0.970263\n",
      "102           102               0                  0                0.264890\n",
      "103           103               0                  0                0.496078\n",
      "104           104               0                  0                0.329749\n",
      "105           105               1                  1                0.745377\n",
      "106           106               1                  1                0.737883\n",
      "107           107               0                  0                0.045131\n",
      "108           108               0                  0                0.325204\n",
      "109           109               0                  0                0.147020\n",
      "110           110               1                  1                0.828969\n",
      "111           111               1                  1                0.700543\n",
      "112           112               0                  1                0.584111\n",
      "113           113               0                  0                0.005669\n",
      "114           114               0                  0                0.006873\n",
      "115           115               0                  0                0.001613\n",
      "116           116               1                  1                0.937080\n",
      "117           117               1                  1                0.708586\n",
      "118           118               1                  1                0.597683\n",
      "119           119               0                  0                0.018260\n",
      "120           120               0                  0                0.002035\n",
      "121           121               1                  1                0.892409\n",
      "122           122               1                  1                0.965850\n",
      "123           123               1                  1                0.929085\n",
      "124           124               1                  1                0.838278\n",
      "125           125               0                  0                0.195210\n",
      "126           126               0                  0                0.163765\n",
      "127           127               1                  0                0.161050\n",
      "128           128               1                  1                0.654435\n",
      "129           129               0                  0                0.013924\n",
      "130           130               0                  0                0.040460\n",
      "131           131               0                  1                0.520141\n",
      "132           132               0                  0                0.126239\n",
      "133           133               0                  1                0.788181\n",
      "134           134               1                  1                0.520232\n",
      "135           135               0                  0                0.260674\n",
      "136           136               0                  0                0.200861\n",
      "137           137               1                  1                0.524408\n",
      "138           138               0                  0                0.498387\n",
      "139           139               0                  0                0.462185\n",
      "140           140               1                  0                0.215088\n",
      "141           141               0                  1                0.547585\n",
      "142           142               0                  1                0.520232\n",
      "143           143               0                  0                0.131416\n"
     ]
    }
   ],
   "source": [
    "red_team, blue_team, won = [], [], []\n",
    "\n",
    "jqd = pd.read_csv(\"../data/frc_data/hopper_qual_data.csv\")\n",
    "\n",
    "iterate = 0\n",
    "\n",
    "for l in range(len(jqd['red_1'])):\n",
    "\n",
    "    red_team.append(scores[jqd[\"red_1\"][iterate]] + scores[jqd[\"red_2\"][iterate]] + scores[jqd[\"red_3\"][iterate]])\n",
    "    blue_team.append(scores[jqd[\"blue_1\"][iterate]] + scores[jqd[\"blue_2\"][iterate]] + scores[jqd[\"blue_3\"][iterate]])\n",
    "    if(jqd['win'][iterate] == \"blue\"):\n",
    "        won.append(1)\n",
    "    else:\n",
    "        won.append(0)\n",
    "    \n",
    "    iterate += 1\n",
    "\n",
    "feature_data = pd.DataFrame({\n",
    "    'red_team': red_team,\n",
    "    'blue_team': blue_team\n",
    "})\n",
    "\n",
    "# Create a Series for the target data\n",
    "target_data = pd.Series(won, name='target')\n",
    "\n",
    "# Combine into one DataFrame if needed\n",
    "combined_data = feature_data.assign(target=target_data)\n",
    "\n",
    "\n",
    "# Extract features and the actual outcome from the new dataset\n",
    "new_X = feature_data  # Use the actual feature names from your new dataset\n",
    "new_y = combined_data['target']  # Use the actual outcome column name from your new dataset\n",
    "\n",
    "# Preprocess the features of the new dataset\n",
    "# (e.g., encoding categorical variables, scaling)\n",
    "# Important: Use the same preprocessing steps as for the training data, and do not refit any preprocessing models\n",
    "new_X_scaled = ss_train.transform(new_X)\n",
    "\n",
    "# Predict outcomes using the pre-trained model\n",
    "new_predictions = model.predict(new_X_scaled)\n",
    "new_prediction_probs = model.predict_proba(new_X_scaled)[:, 1]  # Get the probability of the positive class (assuming '1' is positive)\n",
    "\n",
    "prediction_summary = pd.DataFrame({\n",
    "    'Match Number': feature_data.index,  # or a range if the index is not the match number\n",
    "    'Actual Outcome': new_y,\n",
    "    'Predicted Outcome': new_predictions,\n",
    "    'Probability of Outcome': new_prediction_probs\n",
    "})\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "overall_accuracy = accuracy_score(new_y, new_predictions)\n",
    "print(f'Overall accuracy on the new data: {overall_accuracy:.4f}')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "print(prediction_summary)\n",
    "\n",
    "# Reset the option back to default\n",
    "pd.reset_option('display.max_rows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
