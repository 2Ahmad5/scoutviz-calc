{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/frc_data/2024/test_2024.csv\")\n",
    "\n",
    "scores = {}\n",
    "count = 0\n",
    "for x in df['team']:\n",
    "    \n",
    "    scores[x] = (.182 * df['norm_a_c_s_p'][count]) + (.221 * df['norm_a_g_p'][count]) + (.174 * df['norm_c_p'][count]) + (.222 * df['norm_co_p'][count]) + (.202 * df['norm_eg_p'][count])\n",
    "    count += 1\n",
    "\n",
    "{k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_team, blue_team, won = [], [], []\n",
    "\n",
    "jqd = pd.read_csv(\"../../data/frc_data/2024/all_qual_data.csv\")\n",
    "\n",
    "iterate = 0\n",
    "\n",
    "for l in range(len(jqd['red_1'])):\n",
    "\n",
    "    red_team.append(scores[jqd[\"red_1\"][iterate]] + scores[jqd[\"red_2\"][iterate]] + scores[jqd[\"red_3\"][iterate]])\n",
    "    blue_team.append(scores[jqd[\"blue_1\"][iterate]] + scores[jqd[\"blue_2\"][iterate]] + scores[jqd[\"blue_3\"][iterate]])\n",
    "    if(jqd['win'][iterate] == \"blue\"):\n",
    "        won.append(1)\n",
    "    else:\n",
    "        won.append(0)\n",
    "    \n",
    "    iterate += 1\n",
    "\n",
    "feature_data = pd.DataFrame({\n",
    "    'red_team': red_team,\n",
    "    'blue_team': blue_team\n",
    "})\n",
    "\n",
    "# Create a Series for the target data\n",
    "target_data = pd.Series(won, name='target')\n",
    "\n",
    "# Combine into one DataFrame if needed\n",
    "combined_data = feature_data.assign(target=target_data)\n",
    "\n",
    "# Now combined_data is a DataFrame ready for use in machine learning\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_data\n",
    "y = combined_data['target']\n",
    "\n",
    "combined_data['target'].value_counts()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=.25, random_state=0)\n",
    "\n",
    "ss_train = StandardScaler()\n",
    "X_train = ss_train.fit_transform(X_train)\n",
    "\n",
    "ss_test = StandardScaler()\n",
    "X_test = ss_test.fit_transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prob_predictions = model.predict_proba(X_test)\n",
    "class_predictions = model.predict(X_test)\n",
    "\n",
    "# Loop through the predictions and print required information\n",
    "for i, (prob, prediction, actual) in enumerate(zip(prob_predictions, class_predictions, y_test), start=1):\n",
    "    # Probability of the positive class (assuming it is the second one)\n",
    "    prob_of_one = prob[1] * 100\n",
    "    print(f'Match #{i}: Predicted Class: {prediction}, Actual Class: {actual}, Probability of being 1: {prob_of_one:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, class_predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, class_predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "print('Accuracy of the binary classifier = {:0.3f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
